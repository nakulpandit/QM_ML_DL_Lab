{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 4 - Supervised Learning: Evaluation and diagnostics\n",
    "\n",
    "Example with Planet habitability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "## Index: <a id='index'></a>\n",
    "1. [Research-level datasets](#dataset)\n",
    "2. [Model evaluation](#evaluation)\n",
    "3. [Cross validation](#crossvalidation)\n",
    "4. [Learning curves](#learningcurves)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "## Research-level datasets: [^](#index) <a id='index'></a>\n",
    "\n",
    "Last week we took our first steps in Machine Learning using a small dataset, this week we are getting our hands dirty with a research-level dataset, which will be much larger, messier and noisier than what we saw last week.\n",
    "\n",
    "Let's remind ourselves of the steps of a machine learning project.\n",
    "1. **Problem formulation:** Define the problem that you want to solve. What\n",
    "are you trying to predict or classify? What data do you have available?\n",
    "2. **Data collection:** Collect the data that you need to train the model.\n",
    "This data should be representative of the data that you will use to make\n",
    "predictions.\n",
    "3. **Data preparation and feature engineering:** Prepare the data for training, such as cleaning and\n",
    "transforming it. This may involve removing outliers, imputing missing\n",
    "values, and normalizing the data. Select the features that are most important for the\n",
    "problem. This may involve creating new features or removing irrelevant\n",
    "features.\n",
    "5. **Model selection and training:** Choose the machine learning algorithm that is most\n",
    "suitable for your problem. There are many different machine learning\n",
    "algorithms available, and the best algorithm for your problem will depend\n",
    "on the specific data and the desired outcome. Train the model on the data. This involves feeding the\n",
    "data to the algorithm and letting it learn how to make predictions.\n",
    "7. **Model evaluation:** Evaluate the model on a held-out dataset. This is\n",
    "a dataset that was not used to train the model. The evaluation will help\n",
    "you to assess the accuracy of the model and identify any problems.\n",
    "8. **Model tuning:** Tune the hyperparameters of the model to improve its\n",
    "performance.\n",
    "\n",
    "### STEP 1: Problem formulation \n",
    "Just like last week I will give you the first two steps, but it is a good idea anyway to write out your research question.\n",
    "\n",
    "In the cell below, **define the problem that we want to solve**. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start you will want to run all of these module imports!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PtFRH2i99xOv"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import plot_tree\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import neighbors\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_predict, cross_val_score, cross_validate\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib import colors\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 2: Data collection\n",
    "In the second step, we collect the data that we need to train our model. In this case we use again the [Habitable Worlds Catalogue]('https://phl.upr.edu/hwc'). It lists up to potentially habitable worlds in a list of over five thousand known exoplanets, putting together information gathered by several observatories, including the Kepler and K2 missions and the ongoing Transiting Exoplanet Survey Satellite.\n",
    "\n",
    "Just like you did last week, use the `read_csv` function from the `panda` module to read in the `hwc.csv` file into a panda DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 377
    },
    "executionInfo": {
     "elapsed": 320,
     "status": "ok",
     "timestamp": 1704972145100,
     "user": {
      "displayName": "Linda Cremonesi",
      "userId": "14767435952218470727"
     },
     "user_tz": 0
    },
    "id": "eANzm4IHQjCj",
    "outputId": "0bb451d7-4e61-48a7-b483-3144052c6bdf"
   },
   "outputs": [],
   "source": [
    "pd_exo=pd.read_csv(r'hwc.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bx1xNCSv_ve9"
   },
   "source": [
    "Take a Quick Look at the Data Structure with `.head()`, `describe()`, and `info()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 349
    },
    "executionInfo": {
     "elapsed": 637,
     "status": "ok",
     "timestamp": 1704972148924,
     "user": {
      "displayName": "Linda Cremonesi",
      "userId": "14767435952218470727"
     },
     "user_tz": 0
    },
    "id": "kQ_lxc3V_ve9",
    "outputId": "03e5f755-697d-4de0-a880-6c5c0e004b65"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EJw74bQ-aRTs"
   },
   "source": [
    "This dataset has a lot of features! For the purpose of this exercise, it is useful to just focus on the following interesting features:\n",
    "\n",
    "S_MASS - star mass (solar units)\n",
    "\n",
    "P_DISTANCE - planet mean distance from the star (AU)\n",
    "\n",
    "P_PERIOD - planet period (days)\n",
    "\n",
    "P_FLUX - planet mean stellar flux (earth units)\n",
    "\n",
    "Define a new dataset with just these three features and the labels `P_HABITABLE` and verify using `head()` or `describe()` that it worked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_exo_int = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 3 Data preparation and Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ykMiG3gfYylV"
   },
   "source": [
    "Let's check the `P_HABITABLE` entries. You can use the `groupby` function of `panda` to group the dataframe by the `P_HABITABLE` feature. If you are not sure how to use it, check it out with `help` first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "executionInfo": {
     "elapsed": 213,
     "status": "ok",
     "timestamp": 1704972161920,
     "user": {
      "displayName": "Linda Cremonesi",
      "userId": "14767435952218470727"
     },
     "user_tz": 0
    },
    "id": "NtbvPkaLAfU2",
    "outputId": "37c8b25f-4b71-4932-fa54-307cc29d557e"
   },
   "outputs": [],
   "source": [
    "pd_exo_int..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vNpyJEe3Y51N"
   },
   "source": [
    "The three numbers mean:\n",
    "```\n",
    "0 --> not habitable\n",
    "1 --> conservative\n",
    "2 --> optimistic\n",
    "```\n",
    "It would be more useful for us just to have a binary option, so we should transform our dataset accordingly.\n",
    "\n",
    "Let's start with creating a new dataset without the `P_HABITABLE` feature (hint: use `drop` from `panda`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Diy2cekAYc0o"
   },
   "outputs": [],
   "source": [
    "pd_exo_bin = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tGp32VGdZwQD"
   },
   "source": [
    "Now let's add a column called `P_HABITABLE` to our new data frame, with:\n",
    "\n",
    "```\n",
    "0 --> not habitable\n",
    "1 --> habitable\n",
    "```\n",
    "\n",
    "Now let's add a feature to our dataframe that is the `logical or` of the 1 and 2 `P_HABITABLE` original values (hint: use `logical_or` from `numpy`) \n",
    "\n",
    "NB `logical_or` will return True and False values, so you will also need to convert the column to integers. \n",
    "Once you are confident that you have performed the two operations correctly, check that `pd_exo_bin.head()` gives you the result you expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jimELm7jZmDm"
   },
   "outputs": [],
   "source": [
    "pd_exo_bin['P_HABITABLE'] = ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 377
    },
    "executionInfo": {
     "elapsed": 507,
     "status": "ok",
     "timestamp": 1704972170315,
     "user": {
      "displayName": "Linda Cremonesi",
      "userId": "14767435952218470727"
     },
     "user_tz": 0
    },
    "id": "jJrpTGzCaOji",
    "outputId": "41505998-4106-441f-bae6-0cded90a6f5d"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `numpy` functions `isnan` and `isinf`, check how many instances of NaN (not a number) and infinity are in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 226,
     "status": "ok",
     "timestamp": 1704972806626,
     "user": {
      "displayName": "Linda Cremonesi",
      "userId": "14767435952218470727"
     },
     "user_tz": 0
    },
    "id": "C9Xm58xrty8H",
    "outputId": "83f1b7a8-9c5c-4cd4-f98c-8c2234d0d613"
   },
   "outputs": [],
   "source": [
    "### Counting missing data...\n",
    "countInf = ...\n",
    "countNaN = ...\n",
    "print(\"There are {} instances of NaN in the dataset\".format(countNaN))\n",
    "print(\"There are {} instances of Infinity in the dataset\".format(countInf))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw during the lecture, there are different ways of dealing with missing values. In our case we will choose a shortcut and replace all infinities with NaN, and then drop all of the instances of NaN in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 214,
     "status": "ok",
     "timestamp": 1704972809169,
     "user": {
      "displayName": "Linda Cremonesi",
      "userId": "14767435952218470727"
     },
     "user_tz": 0
    },
    "id": "jNVtsPoCt25E",
    "outputId": "496dddc0-7537-4f2a-99a8-1289193ed6ae"
   },
   "outputs": [],
   "source": [
    "...\n",
    "final_features = ...\n",
    "final_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output from `final_features.shape` should give you `(5263, 5)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to also define our targets as the `P_HABITABLE` column in the dataframe, and drop the `P_HABITABLE` feature from our dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "executionInfo": {
     "elapsed": 351,
     "status": "ok",
     "timestamp": 1704972798007,
     "user": {
      "displayName": "Linda Cremonesi",
      "userId": "14767435952218470727"
     },
     "user_tz": 0
    },
    "id": "TBSvhsgptmI6",
    "outputId": "ace6806a-b7cc-48ea-e3e5-1d2e4dc8bf14"
   },
   "outputs": [],
   "source": [
    "targets = ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_features = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Number 1 rule of science: know your data!** \n",
    "Plot 4 histograms with the final features we chose. Use `describe` to inspect the dataset. Are there any outliers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 717
    },
    "executionInfo": {
     "elapsed": 2224,
     "status": "ok",
     "timestamp": 1704972814491,
     "user": {
      "displayName": "Linda Cremonesi",
      "userId": "14767435952218470727"
     },
     "user_tz": 0
    },
    "id": "iYCbrP3p_ve-",
    "outputId": "17e20ebc-4c44-47ff-c5dc-717f494ff73d"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "executionInfo": {
     "elapsed": 426,
     "status": "ok",
     "timestamp": 1704972820072,
     "user": {
      "displayName": "Linda Cremonesi",
      "userId": "14767435952218470727"
     },
     "user_tz": 0
    },
    "id": "cYk2if8HttU-",
    "outputId": "c32681e5-ae7d-43a7-9085-b704f344dc25"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear that are some big outliers in this dataset that may end up biasing our result. One way to deal with them is to remove all outliers that are more than 5 standard deviations away from the mean.\n",
    "\n",
    "The `stats` `zscore` method gives you a way to evaluate how many sigmas away is a value from the mean. Use it to filter out instances that have features that are more than 5 sigmas away. Then verify using `describe` that the datasets have less outlieers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "920TcWLT1A7V"
   },
   "outputs": [],
   "source": [
    "final_features = ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "executionInfo": {
     "elapsed": 312,
     "status": "ok",
     "timestamp": 1704972865464,
     "user": {
      "displayName": "Linda Cremonesi",
      "userId": "14767435952218470727"
     },
     "user_tz": 0
    },
    "id": "dTdufON617_1",
    "outputId": "34376e19-1da0-4a0a-900e-b39e0536255d"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need to redefine our targets using the new indices. Run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oTCrUVDA1A7V"
   },
   "outputs": [],
   "source": [
    "targets = targets[final_features.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reset the index of both final_features and targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZNx3TVky1A7W"
   },
   "outputs": [],
   "source": [
    "final_features = final_features.reset_index(drop=True)\n",
    "targets = targets.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "## Exercise 1\n",
    "1. Size: What was the size of the data set before and after the cleaning up procedure?\n",
    "2. Missing data: how did you deal with them? what could have been a better way of dealing with them?\n",
    "3. Outliers: how did we deal with them?\n",
    "4. Balance: check how many habitable planets are in the dataset and if it is a similar number to the non-habitable ones\n",
    "5. Intuition: which model is it going to work best, decision trees or kNN? why?\n",
    "6. Do the habitable and non-habitable planets have similar statical features? (hint: use `concat` to put together the `final_features` and `targets` dataframes, and `describe` using the `percintiles=[]` option and `groupby` `P_HABITABLE`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QJIpQEk01A7i"
   },
   "source": [
    "### STEP 4: Model selection and training\n",
    "\n",
    "Use the cell below to define the train and test sets, we use `random_state=2` for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l1j5xrsN1A7j"
   },
   "outputs": [],
   "source": [
    "Xtrain, Xtest, ytrain, ytest = train_test_split(final_features,targets,random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 199,
     "status": "ok",
     "timestamp": 1704972904082,
     "user": {
      "displayName": "Linda Cremonesi",
      "userId": "14767435952218470727"
     },
     "user_tz": 0
    },
    "id": "axXzrd5A1A7j",
    "outputId": "76883880-86b5-4559-84f2-71d9d21c2195"
   },
   "outputs": [],
   "source": [
    "Xtrain.shape, Xtest.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "## STEP 5: Model evaluation [^](#index) <a id='evaluation'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "## Exercise 2\n",
    "1. Plot a scatter graph of the train and test set like you did last week.\n",
    "2. Train a `DecisionTreeClassifier` with `random_state=3`.\n",
    "3. Display the graph of the decision tree and count how many nodes it has. (hint: you can use `node_count` from the tree option)\n",
    "4. Calculate the `accuracy_score`, `precision_score`, and `recall_score` for the train and test set. Write a comment on the performance of the classifier.\n",
    "5. Compare the test set metrics to those you would find with 3 dummy classifiers: one that assigns everything to the habitable class, one that assigns everything to the non-habitable class, and one that is alternating 0s and 1s.\n",
    "6. Display the 4 confusion matrices from the 3 dummy classifier and our decision tree one.\n",
    "7. Which of the classifier performs better? Which has the best accuracy? best precision? best recall? Why? Justify each answer.\n",
    "8. Plot the ROC curve. Comment on the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "## Cross validation [^](#index) <a id='crossvalidation'></a>\n",
    "\n",
    "The idea of cross validation is that we choose several possible train/test splits and repeat the training process accordingly. The overall performance can be estimated as the mean (or median) of the test scores obtained in all the attempts. The standard deviation (or other dispersion measures) provides an estimate of the uncertainty.\n",
    "\n",
    "*k-fold* cross-validation is the most common strategy. It consists of dividing the learning set in *k* folds and cycling through the folds so that at each iteration one is the test set and the remaining *k-1* folds are the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lCopEIaZ1A7n"
   },
   "source": [
    "The 3 cells below show 3 types of Cross Validation:\n",
    " - the *standard version*: it doesn't shuffle the data, so if your positive examples are all at the beginning or all the end, it might lead to disastrous results.\n",
    " - the *shuffle version*: it shuffles the data as well \n",
    " - the *stratification version*: it ensures that the class distributions in each split resembles those of the  entire data set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UNEyVkSZ1A7n"
   },
   "outputs": [],
   "source": [
    "cv1 = KFold(n_splits = 5) # standard\n",
    "\n",
    "cv2 = KFold(shuffle = True, n_splits = 5, random_state=5) # shuffled\n",
    "\n",
    "cv3 = StratifiedKFold(shuffle = True, n_splits = 5, random_state=5) # stratification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J4_J_hXO1A7o"
   },
   "source": [
    "Let's look at the class count in each set of splits for the first cross validation case. Run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l7sQtzMZ1A7o",
    "outputId": "1befcafb-c48b-487e-9898-a8bedf2da4c6"
   },
   "outputs": [],
   "source": [
    "for train, test in cv1.split(final_features, targets): \n",
    "    print('train -  {}   |   test -  {}'.format(np.bincount(targets.loc[train]), np.bincount(targets.loc[test])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P8dLzLaH1A7o"
   },
   "source": [
    "The handy function *cross\\_validate*  provides the scores (specified by the chosen scoring parameter), in dictionary form. I'll do it for the first cross validation and for *accuracy* as the chosen metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores1 = cross_validate(DecisionTreeClassifier(), final_features, targets, cv = cv1, scoring = 'accuracy')\n",
    "scores1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GdVatm0U1A7p"
   },
   "source": [
    "Calculate the mean and the standard deviation of the `test_score` entry in the score dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2s4jFSSp1A7p",
    "outputId": "90526749-4737-499d-806b-0e8f5fd363a5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " If desired, I can ask for the train scores as well and compare them to the test scores. This is very helpful when diagnosing bias vs variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u7Xm45hM1A7q"
   },
   "outputs": [],
   "source": [
    "scores1 = cross_validate(DecisionTreeClassifier(), final_features, targets, cv = cv1, scoring = 'accuracy', \\\n",
    "                         return_train_score = True)\n",
    "print(\"Accuracy train score: {:.2} +/- {:.2}\".format(scores1['train_score'].mean(), scores1['train_score'].std()))\n",
    "print(\"Accuracy test score: {:.2} +/- {:.2}\".format(scores1['test_score'].mean(), scores1['test_score'].std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BVbRhPtx1A7q"
   },
   "source": [
    "The cross\\_validate function is useful to calculate the score, but does not produce predicted labels.\n",
    "\n",
    "These can be obtained by using the `cross_val_predict` function, which saves the predictions for each of the k test folds, and compiles them together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Te-sUv-A1A7r"
   },
   "outputs": [],
   "source": [
    "model1 = DecisionTreeClassifier(random_state=3)\n",
    "scores1 = cross_val_score(model1, final_features, targets, cv = cv1, scoring = 'accuracy')\n",
    "y1 = cross_val_predict(model1, final_features, targets, cv = cv1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QOUbKOuT1A7r",
    "outputId": "542280d3-67d8-4487-b791-51b072368c15"
   },
   "outputs": [],
   "source": [
    "y1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick trick to see how many planets are predicted to be habitable (predicted label = 1) is just to use sum as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YyxwJwrL1A7s",
    "outputId": "f7b85405-cb1c-4639-8f57-13ac61bca540"
   },
   "outputs": [],
   "source": [
    "np.sum(y1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This allows us to get a confusion matrix too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.confusion_matrix(targets,y1)\n",
    "\n",
    "cm = metrics.confusion_matrix(targets,y1, labels=model.classes_)\n",
    "disp = metrics.ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                               display_labels=['Not Habitable','Habitable'])\n",
    "disp.plot()\n",
    "\n",
    "print(\"Number of True Negatives: {:.3f}\".format(cm[0,0]))\n",
    "print(\"Number of True Positives: {:.3f}\".format(cm[1,1]))\n",
    "print(\"Number of False Negatives: {:.3f}\".format(cm[1,0]))\n",
    "print(\"Number of False Positives: {:.3f}\".format(cm[0,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "## Exercise 3\n",
    "1. Look at the class count for the other two types of cross validation, and compare all 3 class counts. Write a comment with your thoughts about them.\n",
    "2. Calculate the scores for the three cross validations using `cross_validate` and find the their mean and standard deviation. Comment on the model performance.\n",
    "3. Now repeat the same as above, but using `recall` as a metric, and then again using `precision` as a metric. Compare the results.\n",
    "4. Compare the cross validation `recall` scores obtained from the train set and from the test set. Comment on the results.\n",
    "5. Calculate the predicted labels for the stratified cross validation case and all 3 metrics (`accuracy`, `precision`, and `recall`) and compare the number of habitable planets found in each case and the confusion matrices.\n",
    "6. Optional: plot the ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gs3ttFuY1A7o"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8sthU9kD1A7q",
    "outputId": "233551c7-8f94-440e-c1e7-6f60330c738a"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V9LYR8iH1A7q"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8sthU9kD1A7q",
    "outputId": "233551c7-8f94-440e-c1e7-6f60330c738a"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u7Xm45hM1A7q"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zCAIhmba1A7q",
    "outputId": "a6003853-c943-4556-b5e0-b1370a8518e1"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x0-VnS2V1A7v"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "## Exercise 4\n",
    "1. Train a `kNearestNeighbour` classifier with `n_neighbors=3`.\n",
    "2. Calculate the `accuracy_score`, `precision_score`, and `recall_score` for the train and test set, and compare them to those from the initial Decision Tree. Write a comment on the performance of the classifier.\n",
    "3. Now scale the dataset using `RobustScaler` and retrain the classifier. How does it perform?\n",
    "4. Define a pipeline using `pipeline = Pipeline([('transformer', scaler), ('estimator', model)])` and use stratified cross validation to evaluate the performance of the model. *Hint: once you have defined a pipeline you can use it in the `cross_validate` functions instead of the `model` input*\n",
    "5. Hyperparameters tuning and optimisation: Optimise the hyperparameter $k$\n",
    "6. Comment on the results, and identify the possible issues with the training.\n",
    "7. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "## Learning curves and hyperparameters tuning [^](#index)<a id='learningcurves'></a>\n",
    "Learning curves are a useful diagnostic tool for supervised models. They are used to estimate how the performance of the model is tied to the size of the learning set. \n",
    "Run the cell below to see an example in which we find the train and test scores for our pipeline when using a sample of increasing size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sizes, train_scores, test_scores = learning_curve(estimator = pipeline, X = final_features, y = targets, \n",
    "    cv = 5, train_sizes=np.linspace(0.1, 1.0, 5), scoring = 'recall')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "## Exercise 5\n",
    "1. Calculate the mean and standard deviation of the train and test scores, and plot them as a function of the train size. You should use a solid line for the mean recall, and a shaded fill area (*hint `fill_between` with a suitable `alpha` option) for marking the error.\n",
    "2. Now repeat the same methods and plot but using a model `DecisionTreeClassifier` like we defined earlier today. Make sure to include a plot title to easily distinguish your figures. Comment on the results.\n",
    "3. It looks like our models are not performing great. Sometimes tuning hyper-parameters helps. Repeat the learning curves but for a `kNN` model with 5 and 7 neighbours. Comment on the results.\n",
    "4. The decision tree we used at the beginning of the lab book had more than 100 nodes, can you get better performance by setting a maximum depth of the tree (*hint using `max_depth`). Use the learning curve to determine how the performance changes with the number of nodes. Comment on the behaviour.\n",
    "\n",
    "Key points: plot Recall vs size of training dataset\n",
    "To find if this is overfitting or underfitting (i.e check by using the training and test scores)\n",
    "Seek optimal fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMkwsRbr6kfahxKh5Bqc6MT",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
