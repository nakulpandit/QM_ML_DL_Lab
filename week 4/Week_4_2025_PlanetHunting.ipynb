{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 4 - Supervised Learning: Evaluation and diagnostics\n",
    "\n",
    "Example with Planet habitability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "## Index: <a id='index'></a>\n",
    "1. [Research-level datasets](#dataset)\n",
    "2. [Model evaluation](#evaluation)\n",
    "3. [Cross validation](#crossvalidation)\n",
    "4. [Learning curves](#learningcurves)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "## Research-level datasets: [^](#index) <a id='index'></a>\n",
    "\n",
    "Last week we took our first steps in Machine Learning using a small dataset, this week we are getting our hands dirty with a research-level dataset, which will be much larger, messier and noisier than what we saw last week.\n",
    "\n",
    "Let's remind ourselves of the steps of a machine learning project.\n",
    "1. **Problem formulation:** Define the problem that you want to solve. What\n",
    "are you trying to predict or classify? What data do you have available?\n",
    "2. **Data collection:** Collect the data that you need to train the model.\n",
    "This data should be representative of the data that you will use to make\n",
    "predictions.\n",
    "3. **Data preparation and feature engineering:** Prepare the data for training, such as cleaning and\n",
    "transforming it. This may involve removing outliers, imputing missing\n",
    "values, and normalizing the data. Select the features that are most important for the\n",
    "problem. This may involve creating new features or removing irrelevant\n",
    "features.\n",
    "5. **Model selection and training:** Choose the machine learning algorithm that is most\n",
    "suitable for your problem. There are many different machine learning\n",
    "algorithms available, and the best algorithm for your problem will depend\n",
    "on the specific data and the desired outcome. Train the model on the data. This involves feeding the\n",
    "data to the algorithm and letting it learn how to make predictions.\n",
    "7. **Model evaluation:** Evaluate the model on a held-out dataset. This is\n",
    "a dataset that was not used to train the model. The evaluation will help\n",
    "you to assess the accuracy of the model and identify any problems.\n",
    "8. **Model tuning:** Tune the hyperparameters of the model to improve its\n",
    "performance.\n",
    "\n",
    "### STEP 1: Problem formulation \n",
    "Just like last week I will give you the first two steps, but it is a good idea anyway to write out your research question.\n",
    "\n",
    "In the cell below, **define the problem that we want to solve**. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - #### We aim to build a supervised classifier that predicts whether an exoplanet is habitable.\n",
    " - #### Data available consists of a researchâ€‘scale exoplanet catalog with many columns.\n",
    " - #### Four predictive features: S_MASS (stellar mass), P_DISTANCE (planet mean orbital distance in AU), P_PERIOD (orbital period in days), and P_FLUX (mean stellar flux in Earth units)\n",
    " - #### one target P_HABITABLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start you will want to run all of these module imports!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "PtFRH2i99xOv"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import plot_tree\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import neighbors\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_predict, cross_val_score, cross_validate\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib import colors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 2: Data collection\n",
    "In the second step, we collect the data that we need to train our model. In this case we use again the [Habitable Worlds Catalogue]('https://phl.upr.edu/hwc'). It lists up to potentially habitable worlds in a list of over five thousand known exoplanets, putting together information gathered by several observatories, including the Kepler and K2 missions and the ongoing Transiting Exoplanet Survey Satellite.\n",
    "\n",
    "Just like you did last week, use the `read_csv` function from the `panda` module to read in the `hwc.csv` file into a panda DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 377
    },
    "executionInfo": {
     "elapsed": 320,
     "status": "ok",
     "timestamp": 1704972145100,
     "user": {
      "displayName": "Linda Cremonesi",
      "userId": "14767435952218470727"
     },
     "user_tz": 0
    },
    "id": "eANzm4IHQjCj",
    "outputId": "0bb451d7-4e61-48a7-b483-3144052c6bdf"
   },
   "outputs": [],
   "source": [
    "pd_exo=pd.read_csv(r'hwc.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bx1xNCSv_ve9"
   },
   "source": [
    "Take a Quick Look at the Data Structure with `.head()`, `describe()`, and `info()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 349
    },
    "executionInfo": {
     "elapsed": 637,
     "status": "ok",
     "timestamp": 1704972148924,
     "user": {
      "displayName": "Linda Cremonesi",
      "userId": "14767435952218470727"
     },
     "user_tz": 0
    },
    "id": "kQ_lxc3V_ve9",
    "outputId": "03e5f755-697d-4de0-a880-6c5c0e004b65"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>P_NAME</th>\n",
       "      <th>P_MASS</th>\n",
       "      <th>P_MASS_ERROR_MIN</th>\n",
       "      <th>P_MASS_ERROR_MAX</th>\n",
       "      <th>P_RADIUS</th>\n",
       "      <th>P_RADIUS_ERROR_MIN</th>\n",
       "      <th>P_RADIUS_ERROR_MAX</th>\n",
       "      <th>P_YEAR</th>\n",
       "      <th>P_UPDATED</th>\n",
       "      <th>P_PERIOD</th>\n",
       "      <th>...</th>\n",
       "      <th>S_ABIO_ZONE</th>\n",
       "      <th>S_TIDAL_LOCK</th>\n",
       "      <th>P_HABZONE_OPT</th>\n",
       "      <th>P_HABZONE_CON</th>\n",
       "      <th>P_TYPE_TEMP</th>\n",
       "      <th>P_HABITABLE</th>\n",
       "      <th>P_ESI</th>\n",
       "      <th>S_CONSTELLATION</th>\n",
       "      <th>S_CONSTELLATION_ABR</th>\n",
       "      <th>S_CONSTELLATION_ENG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>OGLE-2016-BLG-1227L b</td>\n",
       "      <td>251.084120</td>\n",
       "      <td>-123.952920</td>\n",
       "      <td>413.176400</td>\n",
       "      <td>13.90040</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>2020</td>\n",
       "      <td>2020</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Cold</td>\n",
       "      <td>0</td>\n",
       "      <td>0.146639</td>\n",
       "      <td>Scorpius</td>\n",
       "      <td>Sco</td>\n",
       "      <td>Scorpion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Kepler-276 c</td>\n",
       "      <td>16.527056</td>\n",
       "      <td>-3.496108</td>\n",
       "      <td>4.449592</td>\n",
       "      <td>2.90339</td>\n",
       "      <td>-0.28025</td>\n",
       "      <td>1.26673</td>\n",
       "      <td>2013</td>\n",
       "      <td>2014</td>\n",
       "      <td>31.884000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.097783</td>\n",
       "      <td>0.316980</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Hot</td>\n",
       "      <td>0</td>\n",
       "      <td>0.271883</td>\n",
       "      <td>Cygnus</td>\n",
       "      <td>Cyg</td>\n",
       "      <td>Swan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Kepler-829 b</td>\n",
       "      <td>5.085248</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.10748</td>\n",
       "      <td>-0.17936</td>\n",
       "      <td>0.43719</td>\n",
       "      <td>2016</td>\n",
       "      <td>2016</td>\n",
       "      <td>6.883376</td>\n",
       "      <td>...</td>\n",
       "      <td>1.756317</td>\n",
       "      <td>0.459559</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Hot</td>\n",
       "      <td>0</td>\n",
       "      <td>0.254888</td>\n",
       "      <td>Lyra</td>\n",
       "      <td>Lyr</td>\n",
       "      <td>Lyre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>K2-283 b</td>\n",
       "      <td>12.172812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.51994</td>\n",
       "      <td>-0.15694</td>\n",
       "      <td>0.15694</td>\n",
       "      <td>2018</td>\n",
       "      <td>2018</td>\n",
       "      <td>1.921036</td>\n",
       "      <td>...</td>\n",
       "      <td>0.568374</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Hot</td>\n",
       "      <td>0</td>\n",
       "      <td>0.193908</td>\n",
       "      <td>Pisces</td>\n",
       "      <td>Psc</td>\n",
       "      <td>Fishes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Kepler-477 b</td>\n",
       "      <td>4.926334</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.07385</td>\n",
       "      <td>-0.12331</td>\n",
       "      <td>0.17936</td>\n",
       "      <td>2016</td>\n",
       "      <td>2016</td>\n",
       "      <td>11.119907</td>\n",
       "      <td>...</td>\n",
       "      <td>0.768502</td>\n",
       "      <td>0.386150</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Hot</td>\n",
       "      <td>0</td>\n",
       "      <td>0.276524</td>\n",
       "      <td>Lyra</td>\n",
       "      <td>Lyr</td>\n",
       "      <td>Lyre</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 103 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  P_NAME      P_MASS  P_MASS_ERROR_MIN  P_MASS_ERROR_MAX  \\\n",
       "0  OGLE-2016-BLG-1227L b  251.084120       -123.952920        413.176400   \n",
       "1           Kepler-276 c   16.527056         -3.496108          4.449592   \n",
       "2           Kepler-829 b    5.085248          0.000000          0.000000   \n",
       "3               K2-283 b   12.172812          0.000000          0.000000   \n",
       "4           Kepler-477 b    4.926334          0.000000          0.000000   \n",
       "\n",
       "   P_RADIUS  P_RADIUS_ERROR_MIN  P_RADIUS_ERROR_MAX  P_YEAR  P_UPDATED  \\\n",
       "0  13.90040             0.00000             0.00000    2020       2020   \n",
       "1   2.90339            -0.28025             1.26673    2013       2014   \n",
       "2   2.10748            -0.17936             0.43719    2016       2016   \n",
       "3   3.51994            -0.15694             0.15694    2018       2018   \n",
       "4   2.07385            -0.12331             0.17936    2016       2016   \n",
       "\n",
       "    P_PERIOD  ...  S_ABIO_ZONE  S_TIDAL_LOCK  P_HABZONE_OPT  P_HABZONE_CON  \\\n",
       "0   0.000000  ...     0.000046      0.000000              0              0   \n",
       "1  31.884000  ...     2.097783      0.316980              0              0   \n",
       "2   6.883376  ...     1.756317      0.459559              0              0   \n",
       "3   1.921036  ...     0.568374      0.000000              0              0   \n",
       "4  11.119907  ...     0.768502      0.386150              0              0   \n",
       "\n",
       "   P_TYPE_TEMP  P_HABITABLE     P_ESI  S_CONSTELLATION  S_CONSTELLATION_ABR  \\\n",
       "0         Cold            0  0.146639         Scorpius                  Sco   \n",
       "1          Hot            0  0.271883           Cygnus                  Cyg   \n",
       "2          Hot            0  0.254888             Lyra                  Lyr   \n",
       "3          Hot            0  0.193908           Pisces                  Psc   \n",
       "4          Hot            0  0.276524             Lyra                  Lyr   \n",
       "\n",
       "   S_CONSTELLATION_ENG  \n",
       "0             Scorpion  \n",
       "1                 Swan  \n",
       "2                 Lyre  \n",
       "3               Fishes  \n",
       "4                 Lyre  \n",
       "\n",
       "[5 rows x 103 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_exo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.12/site-packages/pandas/core/nanops.py:1016: RuntimeWarning: invalid value encountered in subtract\n",
      "  sqr = _ensure_numeric((avg - values) ** 2)\n",
      "/home/codespace/.local/lib/python3.12/site-packages/pandas/core/nanops.py:1016: RuntimeWarning: invalid value encountered in subtract\n",
      "  sqr = _ensure_numeric((avg - values) ** 2)\n",
      "/home/codespace/.local/lib/python3.12/site-packages/pandas/core/nanops.py:1016: RuntimeWarning: invalid value encountered in subtract\n",
      "  sqr = _ensure_numeric((avg - values) ** 2)\n",
      "/home/codespace/.local/lib/python3.12/site-packages/pandas/core/nanops.py:1016: RuntimeWarning: invalid value encountered in subtract\n",
      "  sqr = _ensure_numeric((avg - values) ** 2)\n",
      "/home/codespace/.local/lib/python3.12/site-packages/pandas/core/nanops.py:1016: RuntimeWarning: invalid value encountered in subtract\n",
      "  sqr = _ensure_numeric((avg - values) ** 2)\n",
      "/home/codespace/.local/lib/python3.12/site-packages/pandas/core/nanops.py:1016: RuntimeWarning: invalid value encountered in subtract\n",
      "  sqr = _ensure_numeric((avg - values) ** 2)\n",
      "/home/codespace/.local/lib/python3.12/site-packages/pandas/core/nanops.py:1016: RuntimeWarning: invalid value encountered in subtract\n",
      "  sqr = _ensure_numeric((avg - values) ** 2)\n",
      "/home/codespace/.local/lib/python3.12/site-packages/pandas/core/nanops.py:1016: RuntimeWarning: invalid value encountered in subtract\n",
      "  sqr = _ensure_numeric((avg - values) ** 2)\n",
      "/home/codespace/.local/lib/python3.12/site-packages/pandas/core/nanops.py:1016: RuntimeWarning: invalid value encountered in subtract\n",
      "  sqr = _ensure_numeric((avg - values) ** 2)\n",
      "/home/codespace/.local/lib/python3.12/site-packages/pandas/core/nanops.py:1016: RuntimeWarning: invalid value encountered in subtract\n",
      "  sqr = _ensure_numeric((avg - values) ** 2)\n",
      "/home/codespace/.local/lib/python3.12/site-packages/pandas/core/nanops.py:1016: RuntimeWarning: invalid value encountered in subtract\n",
      "  sqr = _ensure_numeric((avg - values) ** 2)\n",
      "/home/codespace/.local/lib/python3.12/site-packages/pandas/core/nanops.py:1016: RuntimeWarning: invalid value encountered in subtract\n",
      "  sqr = _ensure_numeric((avg - values) ** 2)\n",
      "/home/codespace/.local/lib/python3.12/site-packages/pandas/core/nanops.py:1016: RuntimeWarning: invalid value encountered in subtract\n",
      "  sqr = _ensure_numeric((avg - values) ** 2)\n",
      "/home/codespace/.local/lib/python3.12/site-packages/pandas/core/nanops.py:1016: RuntimeWarning: invalid value encountered in subtract\n",
      "  sqr = _ensure_numeric((avg - values) ** 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>P_MASS</th>\n",
       "      <th>P_MASS_ERROR_MIN</th>\n",
       "      <th>P_MASS_ERROR_MAX</th>\n",
       "      <th>P_RADIUS</th>\n",
       "      <th>P_RADIUS_ERROR_MIN</th>\n",
       "      <th>P_RADIUS_ERROR_MAX</th>\n",
       "      <th>P_YEAR</th>\n",
       "      <th>P_UPDATED</th>\n",
       "      <th>P_PERIOD</th>\n",
       "      <th>P_PERIOD_ERROR_MIN</th>\n",
       "      <th>...</th>\n",
       "      <th>S_HZ_CON0_MAX</th>\n",
       "      <th>S_HZ_CON1_MIN</th>\n",
       "      <th>S_HZ_CON1_MAX</th>\n",
       "      <th>S_SNOW_LINE</th>\n",
       "      <th>S_ABIO_ZONE</th>\n",
       "      <th>S_TIDAL_LOCK</th>\n",
       "      <th>P_HABZONE_OPT</th>\n",
       "      <th>P_HABZONE_CON</th>\n",
       "      <th>P_HABITABLE</th>\n",
       "      <th>P_ESI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5429.000000</td>\n",
       "      <td>5429.000000</td>\n",
       "      <td>5429.000000</td>\n",
       "      <td>5566.000000</td>\n",
       "      <td>5566.000000</td>\n",
       "      <td>5566.000000</td>\n",
       "      <td>5566.000000</td>\n",
       "      <td>5566.000000</td>\n",
       "      <td>5.566000e+03</td>\n",
       "      <td>5.566000e+03</td>\n",
       "      <td>...</td>\n",
       "      <td>5566.000000</td>\n",
       "      <td>5566.000000</td>\n",
       "      <td>5566.000000</td>\n",
       "      <td>5566.000000</td>\n",
       "      <td>5.566000e+03</td>\n",
       "      <td>5566.000000</td>\n",
       "      <td>5566.000000</td>\n",
       "      <td>5566.000000</td>\n",
       "      <td>5566.000000</td>\n",
       "      <td>5263.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>435.512073</td>\n",
       "      <td>-56.044664</td>\n",
       "      <td>74.822279</td>\n",
       "      <td>5.673366</td>\n",
       "      <td>-0.291506</td>\n",
       "      <td>0.367247</td>\n",
       "      <td>2016.143909</td>\n",
       "      <td>2016.332734</td>\n",
       "      <td>7.684540e+04</td>\n",
       "      <td>-1.964066e+04</td>\n",
       "      <td>...</td>\n",
       "      <td>2.297740</td>\n",
       "      <td>1.232818</td>\n",
       "      <td>2.297740</td>\n",
       "      <td>3.567121</td>\n",
       "      <td>9.798415e+34</td>\n",
       "      <td>0.336715</td>\n",
       "      <td>0.060187</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.025332</td>\n",
       "      <td>0.257500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2371.324125</td>\n",
       "      <td>243.280220</td>\n",
       "      <td>367.353833</td>\n",
       "      <td>5.313856</td>\n",
       "      <td>0.816212</td>\n",
       "      <td>1.320016</td>\n",
       "      <td>4.504200</td>\n",
       "      <td>4.446826</td>\n",
       "      <td>5.390851e+06</td>\n",
       "      <td>1.342383e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>4.503969</td>\n",
       "      <td>2.346731</td>\n",
       "      <td>4.503969</td>\n",
       "      <td>6.620466</td>\n",
       "      <td>5.168606e+36</td>\n",
       "      <td>0.189695</td>\n",
       "      <td>0.237854</td>\n",
       "      <td>0.203949</td>\n",
       "      <td>0.210859</td>\n",
       "      <td>0.134995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-4767.420000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-32.509000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1992.000000</td>\n",
       "      <td>1992.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-1.000000e+08</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001910</td>\n",
       "      <td>0.000911</td>\n",
       "      <td>0.001910</td>\n",
       "      <td>0.002434</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.941067</td>\n",
       "      <td>-14.937916</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.771180</td>\n",
       "      <td>-0.325090</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2014.000000</td>\n",
       "      <td>2014.000000</td>\n",
       "      <td>4.016440e+00</td>\n",
       "      <td>-9.675000e-04</td>\n",
       "      <td>...</td>\n",
       "      <td>1.084775</td>\n",
       "      <td>0.581461</td>\n",
       "      <td>1.084775</td>\n",
       "      <td>1.650019</td>\n",
       "      <td>4.140582e-01</td>\n",
       "      <td>0.278981</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.178469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>8.498721</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.768870</td>\n",
       "      <td>-0.123310</td>\n",
       "      <td>0.134520</td>\n",
       "      <td>2016.000000</td>\n",
       "      <td>2016.000000</td>\n",
       "      <td>1.059206e+01</td>\n",
       "      <td>-5.780500e-05</td>\n",
       "      <td>...</td>\n",
       "      <td>1.624331</td>\n",
       "      <td>0.888340</td>\n",
       "      <td>1.624331</td>\n",
       "      <td>2.605336</td>\n",
       "      <td>1.282764e+00</td>\n",
       "      <td>0.432534</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.267601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>155.735720</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.891400</td>\n",
       "      <td>11.770500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.414770</td>\n",
       "      <td>2020.000000</td>\n",
       "      <td>2020.000000</td>\n",
       "      <td>3.837136e+01</td>\n",
       "      <td>-5.245000e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>2.328600</td>\n",
       "      <td>1.278994</td>\n",
       "      <td>2.328600</td>\n",
       "      <td>3.813851</td>\n",
       "      <td>2.504773e+00</td>\n",
       "      <td>0.463091</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.302495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>89627.496000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7945.700000</td>\n",
       "      <td>77.349000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>68.919080</td>\n",
       "      <td>2023.000000</td>\n",
       "      <td>2023.000000</td>\n",
       "      <td>4.020000e+08</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>120.348830</td>\n",
       "      <td>67.331558</td>\n",
       "      <td>120.348830</td>\n",
       "      <td>214.468620</td>\n",
       "      <td>2.726899e+38</td>\n",
       "      <td>1.003328</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.950567</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 90 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             P_MASS  P_MASS_ERROR_MIN  P_MASS_ERROR_MAX     P_RADIUS  \\\n",
       "count   5429.000000       5429.000000       5429.000000  5566.000000   \n",
       "mean     435.512073        -56.044664         74.822279     5.673366   \n",
       "std     2371.324125        243.280220        367.353833     5.313856   \n",
       "min        0.000000      -4767.420000          0.000000     0.000000   \n",
       "25%        3.941067        -14.937916          0.000000     1.771180   \n",
       "50%        8.498721          0.000000          0.000000     2.768870   \n",
       "75%      155.735720          0.000000         15.891400    11.770500   \n",
       "max    89627.496000          0.000000       7945.700000    77.349000   \n",
       "\n",
       "       P_RADIUS_ERROR_MIN  P_RADIUS_ERROR_MAX       P_YEAR    P_UPDATED  \\\n",
       "count         5566.000000         5566.000000  5566.000000  5566.000000   \n",
       "mean            -0.291506            0.367247  2016.143909  2016.332734   \n",
       "std              0.816212            1.320016     4.504200     4.446826   \n",
       "min            -32.509000            0.000000  1992.000000  1992.000000   \n",
       "25%             -0.325090            0.000000  2014.000000  2014.000000   \n",
       "50%             -0.123310            0.134520  2016.000000  2016.000000   \n",
       "75%              0.000000            0.414770  2020.000000  2020.000000   \n",
       "max              0.000000           68.919080  2023.000000  2023.000000   \n",
       "\n",
       "           P_PERIOD  P_PERIOD_ERROR_MIN  ...  S_HZ_CON0_MAX  S_HZ_CON1_MIN  \\\n",
       "count  5.566000e+03        5.566000e+03  ...    5566.000000    5566.000000   \n",
       "mean   7.684540e+04       -1.964066e+04  ...       2.297740       1.232818   \n",
       "std    5.390851e+06        1.342383e+06  ...       4.503969       2.346731   \n",
       "min    0.000000e+00       -1.000000e+08  ...       0.001910       0.000911   \n",
       "25%    4.016440e+00       -9.675000e-04  ...       1.084775       0.581461   \n",
       "50%    1.059206e+01       -5.780500e-05  ...       1.624331       0.888340   \n",
       "75%    3.837136e+01       -5.245000e-06  ...       2.328600       1.278994   \n",
       "max    4.020000e+08        0.000000e+00  ...     120.348830      67.331558   \n",
       "\n",
       "       S_HZ_CON1_MAX  S_SNOW_LINE   S_ABIO_ZONE  S_TIDAL_LOCK  P_HABZONE_OPT  \\\n",
       "count    5566.000000  5566.000000  5.566000e+03   5566.000000    5566.000000   \n",
       "mean        2.297740     3.567121  9.798415e+34      0.336715       0.060187   \n",
       "std         4.503969     6.620466  5.168606e+36      0.189695       0.237854   \n",
       "min         0.001910     0.002434  0.000000e+00      0.000000       0.000000   \n",
       "25%         1.084775     1.650019  4.140582e-01      0.278981       0.000000   \n",
       "50%         1.624331     2.605336  1.282764e+00      0.432534       0.000000   \n",
       "75%         2.328600     3.813851  2.504773e+00      0.463091       0.000000   \n",
       "max       120.348830   214.468620  2.726899e+38      1.003328       1.000000   \n",
       "\n",
       "       P_HABZONE_CON  P_HABITABLE        P_ESI  \n",
       "count    5566.000000  5566.000000  5263.000000  \n",
       "mean        0.043478     0.025332     0.257500  \n",
       "std         0.203949     0.210859     0.134995  \n",
       "min         0.000000     0.000000     0.000019  \n",
       "25%         0.000000     0.000000     0.178469  \n",
       "50%         0.000000     0.000000     0.267601  \n",
       "75%         0.000000     0.000000     0.302495  \n",
       "max         1.000000     2.000000     0.950567  \n",
       "\n",
       "[8 rows x 90 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_exo.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5566 entries, 0 to 5565\n",
      "Columns: 103 entries, P_NAME to S_CONSTELLATION_ENG\n",
      "dtypes: float64(85), int64(5), object(13)\n",
      "memory usage: 4.4+ MB\n"
     ]
    }
   ],
   "source": [
    "pd_exo.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EJw74bQ-aRTs"
   },
   "source": [
    "This dataset has a lot of features! For the purpose of this exercise, it is useful to just focus on the following interesting features:\n",
    "\n",
    "S_MASS - star mass (solar units)\n",
    "\n",
    "P_DISTANCE - planet mean distance from the star (AU)\n",
    "\n",
    "P_PERIOD - planet period (days)\n",
    "\n",
    "P_FLUX - planet mean stellar flux (earth units)\n",
    "\n",
    "Define a new dataset with just these three features and the labels `P_HABITABLE` and verify using `head()` or `describe()` that it worked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_exo_int = pd_exo[[\"S_MASS\", \"P_DISTANCE\", \"P_PERIOD\",\"P_FLUX\",\"P_HABITABLE\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 3 Data preparation and Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ykMiG3gfYylV"
   },
   "source": [
    "Let's check the `P_HABITABLE` entries. You can use the `groupby` function of `panda` to group the dataframe by the `P_HABITABLE` feature. If you are not sure how to use it, check it out with `help` first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "executionInfo": {
     "elapsed": 213,
     "status": "ok",
     "timestamp": 1704972161920,
     "user": {
      "displayName": "Linda Cremonesi",
      "userId": "14767435952218470727"
     },
     "user_tz": 0
    },
    "id": "NtbvPkaLAfU2",
    "outputId": "37c8b25f-4b71-4932-fa54-307cc29d557e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "P_HABITABLE\n",
       "0    5480\n",
       "1      31\n",
       "2      55\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_exo_int.groupby('P_HABITABLE').size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vNpyJEe3Y51N"
   },
   "source": [
    "The three numbers mean:\n",
    "```\n",
    "0 --> not habitable\n",
    "1 --> conservative\n",
    "2 --> optimistic\n",
    "```\n",
    "It would be more useful for us just to have a binary option, so we should transform our dataset accordingly.\n",
    "\n",
    "Let's start with creating a new dataset without the `P_HABITABLE` feature (hint: use `drop` from `panda`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Diy2cekAYc0o"
   },
   "outputs": [],
   "source": [
    "pd_exo_bin = pd_exo_int.drop(\"P_HABITABLE\", axis = \"columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tGp32VGdZwQD"
   },
   "source": [
    "Now let's add a column called `P_HABITABLE` to our new data frame, with:\n",
    "\n",
    "```\n",
    "0 --> not habitable\n",
    "1 --> habitable\n",
    "```\n",
    "\n",
    "Now let's add a feature to our dataframe that is the `logical or` of the 1 and 2 `P_HABITABLE` original values (hint: use `logical_or` from `numpy`) \n",
    "\n",
    "NB `logical_or` will return True and False values, so you will also need to convert the column to integers. \n",
    "Once you are confident that you have performed the two operations correctly, check that `pd_exo_bin.head()` gives you the result you expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "jimELm7jZmDm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      S_MASS  P_DISTANCE     P_PERIOD      P_FLUX  P_HABITABLE\n",
      "5561    1.02    0.053300     4.458149  348.775190            0\n",
      "5562    0.39    2.440000     0.000000    0.167966            0\n",
      "5563    0.84    0.000000     3.770150         inf            0\n",
      "5564    0.53    1.900000     0.000000    0.277008            0\n",
      "5565    1.21    5.205792  3999.000000    0.123796            0\n",
      "P_HABITABLE\n",
      "0    5480\n",
      "1      86\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "pd_exo_bin[\"P_HABITABLE\"] = np.logical_or(\n",
    "    pd_exo_int[\"P_HABITABLE\"] == 1,\n",
    "    pd_exo_int[\"P_HABITABLE\"] == 2\n",
    ").astype(int)\n",
    "\n",
    "print(pd_exo_bin.tail())\n",
    "print(pd_exo_bin[\"P_HABITABLE\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 377
    },
    "executionInfo": {
     "elapsed": 507,
     "status": "ok",
     "timestamp": 1704972170315,
     "user": {
      "displayName": "Linda Cremonesi",
      "userId": "14767435952218470727"
     },
     "user_tz": 0
    },
    "id": "jJrpTGzCaOji",
    "outputId": "41505998-4106-441f-bae6-0cded90a6f5d"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `numpy` functions `isnan` and `isinf`, check how many instances of NaN (not a number) and infinity are in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 226,
     "status": "ok",
     "timestamp": 1704972806626,
     "user": {
      "displayName": "Linda Cremonesi",
      "userId": "14767435952218470727"
     },
     "user_tz": 0
    },
    "id": "C9Xm58xrty8H",
    "outputId": "83f1b7a8-9c5c-4cd4-f98c-8c2234d0d613"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are S_MASS           0\n",
      "P_DISTANCE       0\n",
      "P_PERIOD         0\n",
      "P_FLUX         303\n",
      "P_HABITABLE      0\n",
      "dtype: int64 instances of NaN in the dataset\n",
      "There are S_MASS         0\n",
      "P_DISTANCE     0\n",
      "P_PERIOD       0\n",
      "P_FLUX         0\n",
      "P_HABITABLE    0\n",
      "dtype: int64 instances of Infinity in the dataset\n"
     ]
    }
   ],
   "source": [
    "### Counting missing data...\n",
    "countInf = np.isnan(pd_exo_bin).sum()\n",
    "countNaN = np.isinf(pd_exo_bin).sum()\n",
    "print(\"There are {} instances of NaN in the dataset\".format(countNaN))\n",
    "print(\"There are {} instances of Infinity in the dataset\".format(countInf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw during the lecture, there are different ways of dealing with missing values. In our case we will choose a shortcut and replace all infinities with NaN, and then drop all of the instances of NaN in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 214,
     "status": "ok",
     "timestamp": 1704972809169,
     "user": {
      "displayName": "Linda Cremonesi",
      "userId": "14767435952218470727"
     },
     "user_tz": 0
    },
    "id": "jNVtsPoCt25E",
    "outputId": "496dddc0-7537-4f2a-99a8-1289193ed6ae"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5263, 5)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_exo_bin.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "pd_exo_bin.dropna(inplace=True)\n",
    "final_features = pd_exo_bin\n",
    "final_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output from `final_features.shape` should give you `(5263, 5)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to also define our targets as the `P_HABITABLE` column in the dataframe, and drop the `P_HABITABLE` feature from our dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "executionInfo": {
     "elapsed": 351,
     "status": "ok",
     "timestamp": 1704972798007,
     "user": {
      "displayName": "Linda Cremonesi",
      "userId": "14767435952218470727"
     },
     "user_tz": 0
    },
    "id": "TBSvhsgptmI6",
    "outputId": "ace6806a-b7cc-48ea-e3e5-1d2e4dc8bf14"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    0\n",
       "2    0\n",
       "3    0\n",
       "4    0\n",
       "Name: P_HABITABLE, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets = pd_exo_bin[\"P_HABITABLE\"]\n",
    "targets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>S_MASS</th>\n",
       "      <th>P_DISTANCE</th>\n",
       "      <th>P_PERIOD</th>\n",
       "      <th>P_FLUX</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.10</td>\n",
       "      <td>3.4000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.086505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.10</td>\n",
       "      <td>0.1994</td>\n",
       "      <td>31.884000</td>\n",
       "      <td>20.490365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.98</td>\n",
       "      <td>0.0678</td>\n",
       "      <td>6.883376</td>\n",
       "      <td>238.528680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.89</td>\n",
       "      <td>0.0291</td>\n",
       "      <td>1.921036</td>\n",
       "      <td>353.357260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.87</td>\n",
       "      <td>0.0911</td>\n",
       "      <td>11.119907</td>\n",
       "      <td>51.163853</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   S_MASS  P_DISTANCE   P_PERIOD      P_FLUX\n",
       "0    0.10      3.4000   0.000000    0.086505\n",
       "1    1.10      0.1994  31.884000   20.490365\n",
       "2    0.98      0.0678   6.883376  238.528680\n",
       "3    0.89      0.0291   1.921036  353.357260\n",
       "4    0.87      0.0911  11.119907   51.163853"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_features = pd_exo_bin.drop(\"P_HABITABLE\", axis = \"columns\")\n",
    "final_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Number 1 rule of science: know your data!** \n",
    "Plot 4 histograms with the final features we chose. Use `describe` to inspect the dataset. Are there any outliers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 717
    },
    "executionInfo": {
     "elapsed": 2224,
     "status": "ok",
     "timestamp": 1704972814491,
     "user": {
      "displayName": "Linda Cremonesi",
      "userId": "14767435952218470727"
     },
     "user_tz": 0
    },
    "id": "iYCbrP3p_ve-",
    "outputId": "17e20ebc-4c44-47ff-c5dc-717f494ff73d"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "executionInfo": {
     "elapsed": 426,
     "status": "ok",
     "timestamp": 1704972820072,
     "user": {
      "displayName": "Linda Cremonesi",
      "userId": "14767435952218470727"
     },
     "user_tz": 0
    },
    "id": "cYk2if8HttU-",
    "outputId": "c32681e5-ae7d-43a7-9085-b704f344dc25"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear that are some big outliers in this dataset that may end up biasing our result. One way to deal with them is to remove all outliers that are more than 5 standard deviations away from the mean.\n",
    "\n",
    "The `stats` `zscore` method gives you a way to evaluate how many sigmas away is a value from the mean. Use it to filter out instances that have features that are more than 5 sigmas away. Then verify using `describe` that the datasets have less outlieers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "920TcWLT1A7V"
   },
   "outputs": [],
   "source": [
    "final_features = ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "executionInfo": {
     "elapsed": 312,
     "status": "ok",
     "timestamp": 1704972865464,
     "user": {
      "displayName": "Linda Cremonesi",
      "userId": "14767435952218470727"
     },
     "user_tz": 0
    },
    "id": "dTdufON617_1",
    "outputId": "34376e19-1da0-4a0a-900e-b39e0536255d"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need to redefine our targets using the new indices. Run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oTCrUVDA1A7V"
   },
   "outputs": [],
   "source": [
    "targets = targets[final_features.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reset the index of both final_features and targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZNx3TVky1A7W"
   },
   "outputs": [],
   "source": [
    "final_features = final_features.reset_index(drop=True)\n",
    "targets = targets.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "## Exercise 1\n",
    "1. Size: What was the size of the data set before and after the cleaning up procedure?\n",
    "2. Missing data: how did you deal with them? what could have been a better way of dealing with them?\n",
    "3. Outliers: how did we deal with them?\n",
    "4. Balance: check how many habitable planets are in the dataset and if it is a similar number to the non-habitable ones\n",
    "5. Intuition: which model is it going to work best, decision trees or kNN? why?\n",
    "6. Do the habitable and non-habitable planets have similar statical features? (hint: use `concat` to put together the `final_features` and `targets` dataframes, and `describe` using the `percintiles=[]` option and `groupby` `P_HABITABLE`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QJIpQEk01A7i"
   },
   "source": [
    "### STEP 4: Model selection and training\n",
    "\n",
    "Use the cell below to define the train and test sets, we use `random_state=2` for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l1j5xrsN1A7j"
   },
   "outputs": [],
   "source": [
    "Xtrain, Xtest, ytrain, ytest = train_test_split(final_features,targets,random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 199,
     "status": "ok",
     "timestamp": 1704972904082,
     "user": {
      "displayName": "Linda Cremonesi",
      "userId": "14767435952218470727"
     },
     "user_tz": 0
    },
    "id": "axXzrd5A1A7j",
    "outputId": "76883880-86b5-4559-84f2-71d9d21c2195"
   },
   "outputs": [],
   "source": [
    "Xtrain.shape, Xtest.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "## STEP 5: Model evaluation [^](#index) <a id='evaluation'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "## Exercise 2\n",
    "1. Plot a scatter graph of the train and test set like you did last week.\n",
    "2. Train a `DecisionTreeClassifier` with `random_state=3`.\n",
    "3. Display the graph of the decision tree and count how many nodes it has. (hint: you can use `node_count` from the tree option)\n",
    "4. Calculate the `accuracy_score`, `precision_score`, and `recall_score` for the train and test set. Write a comment on the performance of the classifier.\n",
    "5. Compare the test set metrics to those you would find with 3 dummy classifiers: one that assigns everything to the habitable class, one that assigns everything to the non-habitable class, and one that is alternating 0s and 1s.\n",
    "6. Display the 4 confusion matrices from the 3 dummy classifier and our decision tree one.\n",
    "7. Which of the classifier performs better? Which has the best accuracy? best precision? best recall? Why? Justify each answer.\n",
    "8. Plot the ROC curve. Comment on the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "## Cross validation [^](#index) <a id='crossvalidation'></a>\n",
    "\n",
    "The idea of cross validation is that we choose several possible train/test splits and repeat the training process accordingly. The overall performance can be estimated as the mean (or median) of the test scores obtained in all the attempts. The standard deviation (or other dispersion measures) provides an estimate of the uncertainty.\n",
    "\n",
    "*k-fold* cross-validation is the most common strategy. It consists of dividing the learning set in *k* folds and cycling through the folds so that at each iteration one is the test set and the remaining *k-1* folds are the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lCopEIaZ1A7n"
   },
   "source": [
    "The 3 cells below show 3 types of Cross Validation:\n",
    " - the *standard version*: it doesn't shuffle the data, so if your positive examples are all at the beginning or all the end, it might lead to disastrous results.\n",
    " - the *shuffle version*: it shuffles the data as well \n",
    " - the *stratification version*: it ensures that the class distributions in each split resembles those of the  entire data set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UNEyVkSZ1A7n"
   },
   "outputs": [],
   "source": [
    "cv1 = KFold(n_splits = 5) # standard\n",
    "\n",
    "cv2 = KFold(shuffle = True, n_splits = 5, random_state=5) # shuffled\n",
    "\n",
    "cv3 = StratifiedKFold(shuffle = True, n_splits = 5, random_state=5) # stratification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J4_J_hXO1A7o"
   },
   "source": [
    "Let's look at the class count in each set of splits for the first cross validation case. Run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l7sQtzMZ1A7o",
    "outputId": "1befcafb-c48b-487e-9898-a8bedf2da4c6"
   },
   "outputs": [],
   "source": [
    "for train, test in cv1.split(final_features, targets): \n",
    "    print('train -  {}   |   test -  {}'.format(np.bincount(targets.loc[train]), np.bincount(targets.loc[test])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P8dLzLaH1A7o"
   },
   "source": [
    "The handy function *cross\\_validate*  provides the scores (specified by the chosen scoring parameter), in dictionary form. I'll do it for the first cross validation and for *accuracy* as the chosen metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores1 = cross_validate(DecisionTreeClassifier(), final_features, targets, cv = cv1, scoring = 'accuracy')\n",
    "scores1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GdVatm0U1A7p"
   },
   "source": [
    "Calculate the mean and the standard deviation of the `test_score` entry in the score dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2s4jFSSp1A7p",
    "outputId": "90526749-4737-499d-806b-0e8f5fd363a5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " If desired, I can ask for the train scores as well and compare them to the test scores. This is very helpful when diagnosing bias vs variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u7Xm45hM1A7q"
   },
   "outputs": [],
   "source": [
    "scores1 = cross_validate(DecisionTreeClassifier(), final_features, targets, cv = cv1, scoring = 'accuracy', \\\n",
    "                         return_train_score = True)\n",
    "print(\"Accuracy train score: {:.2} +/- {:.2}\".format(scores1['train_score'].mean(), scores1['train_score'].std()))\n",
    "print(\"Accuracy test score: {:.2} +/- {:.2}\".format(scores1['test_score'].mean(), scores1['test_score'].std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BVbRhPtx1A7q"
   },
   "source": [
    "The cross\\_validate function is useful to calculate the score, but does not produce predicted labels.\n",
    "\n",
    "These can be obtained by using the `cross_val_predict` function, which saves the predictions for each of the k test folds, and compiles them together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Te-sUv-A1A7r"
   },
   "outputs": [],
   "source": [
    "model1 = DecisionTreeClassifier(random_state=3)\n",
    "scores1 = cross_val_score(model1, final_features, targets, cv = cv1, scoring = 'accuracy')\n",
    "y1 = cross_val_predict(model1, final_features, targets, cv = cv1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QOUbKOuT1A7r",
    "outputId": "542280d3-67d8-4487-b791-51b072368c15"
   },
   "outputs": [],
   "source": [
    "y1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick trick to see how many planets are predicted to be habitable (predicted label = 1) is just to use sum as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YyxwJwrL1A7s",
    "outputId": "f7b85405-cb1c-4639-8f57-13ac61bca540"
   },
   "outputs": [],
   "source": [
    "np.sum(y1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This allows us to get a confusion matrix too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.confusion_matrix(targets,y1)\n",
    "\n",
    "cm = metrics.confusion_matrix(targets,y1, labels=model.classes_)\n",
    "disp = metrics.ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                               display_labels=['Not Habitable','Habitable'])\n",
    "disp.plot()\n",
    "\n",
    "print(\"Number of True Negatives: {:.3f}\".format(cm[0,0]))\n",
    "print(\"Number of True Positives: {:.3f}\".format(cm[1,1]))\n",
    "print(\"Number of False Negatives: {:.3f}\".format(cm[1,0]))\n",
    "print(\"Number of False Positives: {:.3f}\".format(cm[0,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "## Exercise 3\n",
    "1. Look at the class count for the other two types of cross validation, and compare all 3 class counts. Write a comment with your thoughts about them.\n",
    "2. Calculate the scores for the three cross validations using `cross_validate` and find the their mean and standard deviation. Comment on the model performance.\n",
    "3. Now repeat the same as above, but using `recall` as a metric, and then again using `precision` as a metric. Compare the results.\n",
    "4. Compare the cross validation `recall` scores obtained from the train set and from the test set. Comment on the results.\n",
    "5. Calculate the predicted labels for the stratified cross validation case and all 3 metrics (`accuracy`, `precision`, and `recall`) and compare the number of habitable planets found in each case and the confusion matrices.\n",
    "6. Optional: plot the ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gs3ttFuY1A7o"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8sthU9kD1A7q",
    "outputId": "233551c7-8f94-440e-c1e7-6f60330c738a"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V9LYR8iH1A7q"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8sthU9kD1A7q",
    "outputId": "233551c7-8f94-440e-c1e7-6f60330c738a"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u7Xm45hM1A7q"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zCAIhmba1A7q",
    "outputId": "a6003853-c943-4556-b5e0-b1370a8518e1"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x0-VnS2V1A7v"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "## Exercise 4\n",
    "1. Train a `kNearestNeighbour` classifier with `n_neighbors=3`.\n",
    "2. Calculate the `accuracy_score`, `precision_score`, and `recall_score` for the train and test set, and compare them to those from the initial Decision Tree. Write a comment on the performance of the classifier.\n",
    "3. Now scale the dataset using `RobustScaler` and retrain the classifier. How does it perform?\n",
    "4. Define a pipeline using `pipeline = Pipeline([('transformer', scaler), ('estimator', model)])` and use stratified cross validation to evaluate the performance of the model. *Hint: once you have defined a pipeline you can use it in the `cross_validate` functions instead of the `model` input*\n",
    "5. Hyperparameters tuning and optimisation: Optimise the hyperparameter $k$\n",
    "6. Comment on the results, and identify the possible issues with the training.\n",
    "7. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "## Learning curves and hyperparameters tuning [^](#index)<a id='learningcurves'></a>\n",
    "Learning curves are a useful diagnostic tool for supervised models. They are used to estimate how the performance of the model is tied to the size of the learning set. \n",
    "Run the cell below to see an example in which we find the train and test scores for our pipeline when using a sample of increasing size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sizes, train_scores, test_scores = learning_curve(estimator = pipeline, X = final_features, y = targets, \n",
    "    cv = 5, train_sizes=np.linspace(0.1, 1.0, 5), scoring = 'recall')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "## Exercise 5\n",
    "1. Calculate the mean and standard deviation of the train and test scores, and plot them as a function of the train size. You should use a solid line for the mean recall, and a shaded fill area (*hint `fill_between` with a suitable `alpha` option) for marking the error.\n",
    "2. Now repeat the same methods and plot but using a model `DecisionTreeClassifier` like we defined earlier today. Make sure to include a plot title to easily distinguish your figures. Comment on the results.\n",
    "3. It looks like our models are not performing great. Sometimes tuning hyper-parameters helps. Repeat the learning curves but for a `kNN` model with 5 and 7 neighbours. Comment on the results.\n",
    "4. The decision tree we used at the beginning of the lab book had more than 100 nodes, can you get better performance by setting a maximum depth of the tree (*hint using `max_depth`). Use the learning curve to determine how the performance changes with the number of nodes. Comment on the behaviour.\n",
    "\n",
    "Key points: plot Recall vs size of training dataset\n",
    "To find if this is overfitting or underfitting (i.e check by using the training and test scores)\n",
    "Seek optimal fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMkwsRbr6kfahxKh5Bqc6MT",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
