{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab Report 1 - Our first Machine Learning Project (Week 2 & 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "## Index: <a id='index'></a>\n",
    "1. [Building a Machine Learning Model](#model)\n",
    "1. [The dataset](#dataset)\n",
    "1. [Decision Trees](#DT)\n",
    "1. [Expectations](#expectations)\n",
    "1. [Nearest Neighbours](#knn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "## Section 1: Building a Machine Learning Model  [^](#index) <a id='model'></a>\n",
    "\n",
    "It is now time to build our very first machine learning model! First of all we need to understand what building a model means. Last week, we saw that the machine learning process involves the following steps:\n",
    "1. **Problem formulation:** Define the problem that you want to solve. What\n",
    "are you trying to predict or classify? What data do you have available?\n",
    "2. **Data collection:** Collect the data that you need to train the model.\n",
    "This data should be representative of the data that you will use to make\n",
    "predictions.\n",
    "3. **Data preparation and feature engineering:** Prepare the data for training, such as cleaning and\n",
    "transforming it. This may involve removing outliers, imputing missing\n",
    "values, and normalizing the data. Select the features that are most important for the\n",
    "problem. This may involve creating new features or removing irrelevant\n",
    "features.\n",
    "5. **Model selection and training:** Choose the machine learning algorithm that is most\n",
    "suitable for your problem. There are many different machine learning\n",
    "algorithms available, and the best algorithm for your problem will depend\n",
    "on the specific data and the desired outcome. Train the model on the data. This involves feeding the\n",
    "data to the algorithm and letting it learn how to make predictions.\n",
    "7. **Model evaluation:** Evaluate the model on a held-out dataset. This is\n",
    "a dataset that was not used to train the model. The evaluation will help\n",
    "you to assess the accuracy of the model and identify any problems.\n",
    "8. **Model tuning:** Tune the hyperparameters of the model to improve its\n",
    "performance.\n",
    "\n",
    "Today it's your lucky day as I will give you point 1 and point 2, and we will focus a lot of this notebook on understanding point 3 to point 6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Problem formulation\n",
    "\n",
    "Today we want to search for intelligent life beyond Earth. We want to look for planets that could host human life. This exercise was mostly taken from MLPA (\"Machine Learning for Physics and Astronomy\" by Viviana Aquaviva), although the dataset is slightly different. You can check out chapter 2 of that book more info on the exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, **define the problem that we want to solve**. Are we trying to predict? Or classify? Do you think we should use supervised learning or unsupervised learning for this task?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Along with our problem formulation, I think we realise that we will need a bunch of python modules to perform our project. I like to import them all at the beginning as it only needs to be done once per session, and I also like to group them thematically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PtFRH2i99xOv"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import plot_tree\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib import colors\n",
    "\n",
    "import UsedForMCQs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sz-MkQli_ve8"
   },
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "## Section 2: The dataset [^](#index) <a id='dataset'></a>\n",
    "\n",
    "### 2.1 Find the appropriate dataset\n",
    "In the second step, we collect the data that we need to train our model. In this case we were lucky and the [Habitable Worlds Catalogue]('https://phl.upr.edu/hwc') has done this for us already. It lists up to potentially habitable worlds in a list of over five thousand known exoplanets, putting together information gathered by several observatories, including the Kepler and K2 missions and the ongoing Transiting Exoplanet Survey Satellite.\n",
    "\n",
    "As this is our very first machine learning project, it may be daunting to look at a 5000+ dataset with tens of features, so I made smaller set for you, made of 18 instances, 3 features and our target labels. This is in the csv file called `HabPlanets_simple.csv`.\n",
    "\n",
    "### 2.2 Read the dataset\n",
    "In week 1, you learned to use the `read_csv` function from the `panda` module, so I left the cell below for you to complete:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 377
    },
    "executionInfo": {
     "elapsed": 320,
     "status": "ok",
     "timestamp": 1704972145100,
     "user": {
      "displayName": "Linda Cremonesi",
      "userId": "14767435952218470727"
     },
     "user_tz": 0
    },
    "id": "eANzm4IHQjCj",
    "outputId": "0bb451d7-4e61-48a7-b483-3144052c6bdf"
   },
   "outputs": [],
   "source": [
    "LearningSet=..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bx1xNCSv_ve9"
   },
   "source": [
    "### 2.3 Check that the dataset has been read correctly\n",
    "\n",
    "Check that your dataset has been read correctly by exploring its structure (displaying the whole `LearningSet`, and using `head()` or `describe(`). **NB You shouldn't be plotting the dataset at this stage as you have not split it into the training and test set yet.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 349
    },
    "executionInfo": {
     "elapsed": 637,
     "status": "ok",
     "timestamp": 1704972148924,
     "user": {
      "displayName": "Linda Cremonesi",
      "userId": "14767435952218470727"
     },
     "user_tz": 0
    },
    "id": "kQ_lxc3V_ve9",
    "outputId": "03e5f755-697d-4de0-a880-6c5c0e004b65"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Understand the features \n",
    "The dataset includes 3 features and one target. Looking at the website source we can see that the column features refer to:\n",
    " - S_MASS - star mass (solar units).\n",
    " - P_PERIOD - planet period (days).\n",
    " - P_DISTANCE - planet mean distance from the star (AU).\n",
    " - P_HABITABLE - boolean variable telling us if the planet is habitable or not.\n",
    "\n",
    "You can change the column names to something handier, or keep them as they are, the important thing is that you remember what they mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "## Exercise 1\n",
    "When dealing with a new dataset it's useful to answer these questions.\n",
    "1. What's the size of the dataset?\n",
    "2. Are there any missing data? if yes, how should you handle them?\n",
    "3. Are all the features in a similar numerical range and is there anything unusual about the distribution of the numerical values?\n",
    "4. Is the dataset imbalanced (ie one or more classes are much more heavily populated than others)?\n",
    "5. Start developing some intuition on how well you expect the model to work: are these features meaningful? do we have enough samples?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QJIpQEk01A7i"
   },
   "source": [
    "## 1.3 Data preparation and feature engineering\n",
    "\n",
    "This step involves preparing the data for training, such as cleaning and transforming it. This may involve removing outliers, imputing missing values, and normalising the data. Select the features that are most important for the problem. This may involve creating new features or removing irrelevant features.\n",
    "\n",
    "### 1.3.1 Splitting between the Train and Test set\n",
    "The first thing we will want to do is split the data set into training and test sets. Normally the train/test split choice happens at random, but for this notebook we will choose a specific split so that the results are reproducible.\n",
    "Use the first 13 instances of the dataframe as a train set and the last 5 as a test set (*hint: you could use the `panda` method `iloc` for this)*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l1j5xrsN1A7j"
   },
   "outputs": [],
   "source": [
    "TrainSet = ...\n",
    "TestSet = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create `Xtrain` and `Xtest` sets which will not have the name and habitable columns (*hint: you can use `drop` to do this*). And create your label sets `ytrain` and `ytest` which will only include the habitable column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain = ...\n",
    "Xtest = ...\n",
    "ytrain = ...\n",
    "ytest  = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that the `shape` of `Xtrain`, `Xtest`, `ytrain`, `ytest` is what you expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 199,
     "status": "ok",
     "timestamp": 1704972904082,
     "user": {
      "displayName": "Linda Cremonesi",
      "userId": "14767435952218470727"
     },
     "user_tz": 0
    },
    "id": "axXzrd5A1A7j",
    "outputId": "76883880-86b5-4559-84f2-71d9d21c2195"
   },
   "outputs": [],
   "source": [
    "Xtrain.shape, Xtest.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QROXGJ7D1A7j"
   },
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "## Exercise 2\n",
    "Plot the train and test set in a nice scatter graph. I added some bits of code which I know will make the plot prettier once you have correctly defined everything. We want the following features:\n",
    "1. Plot a scatter graph of the `Xtrain` dataset (mass of parent star on the x-axis and the orbital period on the y axis), and using the `ytrain` as the `c` option for the colormap (`cmap` has already been defined for you below), `*` as markers, and an `alpha` of 0.5. We will want to label this `Train`.\n",
    "2. Add a scatter graph of the `Xtest` dataset (mass of parent star on the x-axis and the orbital period on the y axis), and using the `ytest` as the `c` option for the colormap, `o` as markers, and an `alpha` of 0.5. We will want to label this `Test`.\n",
    "3. Add descriptive axis labels (including units)\n",
    "4. The y axis should be in a logarithmic scale\n",
    "5. Plot the legend\n",
    "\n",
    "3. Using `plt.axvline` and `plt.axhline` plot a horizontal line at 3.5 and a vertical line at 0.5.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 2915,
     "status": "ok",
     "timestamp": 1704973932176,
     "user": {
      "displayName": "Linda Cremonesi",
      "userId": "14767435952218470727"
     },
     "user_tz": 0
    },
    "id": "21LZWtZg1A7j",
    "outputId": "573d06e7-9719-4fcd-ac64-82d7f3dd2eed"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "cmap = colors.ListedColormap(['purple', 'green'])\n",
    "\n",
    "...\n",
    "\n",
    "purplepatch = mpatches.Patch(color='purple', label='Not Habitable')\n",
    "greenpatch = mpatches.Patch(color='green', label='Habitable')\n",
    "\n",
    "ax = plt.gca()\n",
    "leg = ax.get_legend()\n",
    "leg.legend_handles[0].set_color('k')\n",
    "leg.legend_handles[1].set_color('k')\n",
    "plt.legend(handles=[leg.legend_handles[0],leg.legend_handles[1], purplepatch, greenpatch])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "## Section 3: Decision Trees  [^](#index) <a id='DT'></a>\n",
    "\n",
    "\n",
    "A decision tree is a type of machine learning algorithm that uses a tree-like structure to make decisions or predictions. It's often used in supervised learning tasks, (where you have a dataset with labeled examples and you want to learn a model that can predict the labels for new, unseen examples).\n",
    "\n",
    "Here's how a decision tree works:\n",
    " - **Start at the root node.** This node represents the entire dataset.\n",
    " - **Ask a question about one of the features in the data.** The answer to the question will determine which branch of the tree to take.\n",
    " - **Continue asking questions and following branches until you reach a leaf node.** The leaf node represents a prediction or classification.\n",
    "\n",
    "A good decision is characterised by efficient splits, which has the maximum information gain or maximum decrease of impurity. A metric that is often is used is the **Gini impurity** defined as:\n",
    "$$\n",
    " I_G = 1 - \\sum_i f(i)^2\n",
    "$$\n",
    "where $f(i)$ is the fractional abundance of each class.\n",
    "\n",
    "To calculate if a split is convenient or not, we need to perform 3 steps:\n",
    "1. Calculate the Gini impurity of the current dataset.\n",
    "2. Calculate the Gini impurity of the proposed split.\n",
    "3. Calculate the difference between the two.\n",
    "\n",
    "The largest decrease in impurity will be the preferable option. **NB. The Gini impurity of a proposed split is the sum of the fractional impurities of the two resulting nodes, weighted by the fractional volume of each node with respect to its parent node.**\n",
    "\n",
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "## Exercise 3\n",
    "Using the two lines defined in the scatter plot above and the definition of the Gini impurity, assess whether it is more convenient to split the **train** dataset vertically and then horizontally or the other way round. When calculting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model!\n",
    "It's time to train our Decision Tree and see if our model finds our same results. The following cells does two things:\n",
    " - It defines our model as our decision tree classifier\n",
    " - It then trains the model with out train set\n",
    "The `random_state` variable in this case is set to a specific value for reproducibility purposes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 75
    },
    "executionInfo": {
     "elapsed": 631,
     "status": "ok",
     "timestamp": 1704973387588,
     "user": {
      "displayName": "Linda Cremonesi",
      "userId": "14767435952218470727"
     },
     "user_tz": 0
    },
    "id": "CHd_hfFX1A7k",
    "outputId": "585f30f6-97ab-4aab-bb7a-b352c3300b52"
   },
   "outputs": [],
   "source": [
    "model = DecisionTreeClassifier(random_state=3)\n",
    "model.fit(Xtrain,ytrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "emzs5EuC1A7k"
   },
   "source": [
    "#### Let's visualize the graph!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tree(model, feature_names=['Stellar Mass (M*)', 'Orbital Period (d)', 'Distance (AU)'], \n",
    "          class_names=['Not Habitable','Habitable'], filled = True, rounded = True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zguGgjVW1A7l"
   },
   "source": [
    "These numbers are a little bit different from what we found above. Can you guess why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because we only looked at 2 features, whereas the DT is looking at 3 features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zguGgjVW1A7l"
   },
   "source": [
    "### Let's take a look at some metrics.\n",
    "Using the `model.predict` function, apply the model to `Xtest` and calculate our prediction on the test set and on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytestpred = ...\n",
    "ytrainpred = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `metrics` module you can calculate the `accuracy_score` and compare the performance of the two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 193
    },
    "executionInfo": {
     "elapsed": 229,
     "status": "error",
     "timestamp": 1704974742242,
     "user": {
      "displayName": "Linda Cremonesi",
      "userId": "14767435952218470727"
     },
     "user_tz": 0
    },
    "id": "Op3ELHmg1A7l",
    "outputId": "94395968-f93b-4a8f-aafa-fadfadeb6f1c"
   },
   "outputs": [],
   "source": [
    "test_accuracy  = ...\n",
    "train_accuracy = ...\n",
    "print(\"The accuracy of the test set is {:.3f}\".format(test_accuracy))\n",
    "print(\"The accuracy of the train set is {:.3f}\".format(train_accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cells make a pretty Confusion Matrix and print out the number of true negatives, true positives, false negatives and false positives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = metrics.confusion_matrix(ytest,ytestpred, labels=model.classes_)\n",
    "disp = metrics.ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                               display_labels=['Not Habitable','Habitable'])\n",
    "disp.plot()\n",
    "\n",
    "print(\"Number of True Negatives: {:.3f}\".format(cm[0,0]))\n",
    "print(\"Number of True Positives: {:.3f}\".format(cm[1,1]))\n",
    "print(\"Number of False Negatives: {:.3f}\".format(cm[1,0]))\n",
    "print(\"Number of False Positives: {:.3f}\".format(cm[0,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "## Exercise 4\n",
    "Repeat the same exercise but with taking the first 5 instances of the `LearningSet` as our test set and the last 13 as our training set:\n",
    "1. Plot a scatter graph of the new train set and test set.\n",
    "2. Train the new model (using again `random_state=3` to have reproducibility)\n",
    "3. Visualise the decision tree\n",
    "4. Calculate and display the new accuracy\n",
    "5. Discuss which training is better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "## Section 4: Nearest Neighbours  [^](#index) <a id='neighbor'></a>\n",
    "\n",
    "The nearest neighbour method, also known as the k-nearest neighbours (k-NN) algorithm, is a simple yet powerful technique in machine learning used for both classification and regression tasks. It works on the fundamental assumption that similar data points are likely to have similar labels or values.\n",
    "We can use it in a similar way just by calling the classifier from `scikit-learn`. In this case we use the `KNeighborsClassifier`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "## Exercise 5\n",
    "\n",
    "1. Define the new model as the `KNeighborsClassifier` using the option of `n_neighbor=3` (we use just 3 neighbors as this is a very small dataset, the default is 5 neighbours). Train the dataset using the `fit` method as you've done previously.\n",
    "2. Use the `predict` method from the model to get the predictions and calculate the accuracy scores.\n",
    "3. Plot the confusion matrix and print out the number of true positives, true negatives, false positives, false negatives.\n",
    "4. What do you think about this classifier? did it work well?\n",
    "5. Plot the scatter graph of the test and train set again (yes the usual one!) but without the logarithmic y-axis. Then, use the code below to plot the 5 circles representing the circle of the closest 3 instances to the 5 test points. (Yes, I am giving you the code for this one!). Does this explain the results of the training?\n",
    "\n",
    "```\n",
    "dist, ind = model.kneighbors(Xtest)\n",
    "\n",
    "for index in range(5):\n",
    "    x0 = TestSet.loc[index, 'S_MASS']\n",
    "    y0 = TestSet.loc[index, 'P_PERIOD']\n",
    "    r0 = dist[index].max()\n",
    "    circle=plt.Circle((x0, y0), r0, color='r', fill=False)\n",
    "    ax = plt.gca()\n",
    "    ax.add_patch(circle)\n",
    "\n",
    "plt.xlim(-10, 10)\n",
    "plt.show()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The training performed better as the features now weigh a similar amount and the dataset is not skewed towards the features with highest numerical values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Preprocessing and Scaling\n",
    "\n",
    "Hopefully you have now noticed that one of our features has much larger numerical values than the others, so it takes more weight in the machine learning process. Note that this was not a problem for the Decision Tree, as the decisions were made one at a time.\n",
    "\n",
    "There are a few different options to define a scaler as you have seen in the notes. We will start with a `RobustScaler`, then we use the `fit` method to compute the median and quartiles of the set and scale the set so that the median in 0 and the quartiles are appropriately distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preprocessing.RobustScaler()\n",
    "scaler.fit(Xtrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To apply this transformation, ie to *scale* the training data, we use the `transform` method of the scaler. The `transform` method is used in `scikit-learn` whenever a model returns a new representation of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaledXtrain = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Print the dataset properties** (median, 0.25, 0.75 quantiles) before and after the scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transformed data has the same shape as the original data - the features are simply shifted and scaled.\n",
    "\n",
    "To apply the kNN to the scaled data we need to **apply** the same transformation to the test set as well. **It is important not to use the test set to make the transformation as we don't want to *see* the test set statistical properties**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaledXtest  = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print test set properties before and after the scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "## Exercise 6\n",
    "\n",
    "1. Retrain the neighbour classifier with your new scaled training set. \n",
    "2. Calculate the new accuracy.\n",
    "3. Calculate the new confusion matrix and true positives/negatives, false positives/negatives.\n",
    "4. Remake the scatter plot with the circles.\n",
    "5. Write a short sentence with your thoughts on the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMkwsRbr6kfahxKh5Bqc6MT",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
